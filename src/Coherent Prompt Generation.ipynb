{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb9c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "## self-modified\n",
    "from pipeline_base_modified import PromptDataLoader, PromptForClassification\n",
    "from template_generation_wzy import LMBFFTemplateGenerationTemplate, T5TemplateGenerator\n",
    "from openprompt.plms import load_plm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "cuda = True\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15d8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DF_Processor(DataProcessor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.labels = ['0', '1']\n",
    "\n",
    "    def get_examples(self, df):\n",
    "        examples = []\n",
    "        for i,r in df.iterrows():\n",
    "            text_a = r['utterance']\n",
    "            label = r['labels']\n",
    "            guid = i\n",
    "            example = InputExample(guid=guid, text_a=text_a, label=label)\n",
    "            examples.append(example)\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c6b3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def fit(model, train_dataloader, val_dataloader, loss_func, optimizer):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(10):\n",
    "        train_epoch(model, train_dataloader, loss_func, optimizer)\n",
    "        score = evaluate(model, val_dataloader)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "    return best_score\n",
    "\n",
    "def train_epoch(model, train_dataloader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def template_post_process(template_texts):\n",
    "    results = []\n",
    "    special_tokens = ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>']\n",
    "    for template in template_texts:\n",
    "\n",
    "        new_tmp = []\n",
    "        for i in range(len(template)):\n",
    "            new_tmp.append(template[i])\n",
    "            try:\n",
    "                if template[i] in special_tokens and template[i+1] in special_tokens:\n",
    "                    new_tmp.append('')\n",
    "            except:\n",
    "                pass\n",
    "        results.append(new_tmp)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e008ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "dataset   = 'Friends'\n",
    "data_path = '../data/FriendsPersona/Friends_'\n",
    "\n",
    "\n",
    "\n",
    "beam_width = 10\n",
    "n = 10    # top-n label words for template generation\n",
    "\n",
    "for personality in ['A','C','E','O','N']:\n",
    "    print('Processing: ', personality)\n",
    "    \n",
    "    model_name = 'Adapted_t5_large_'+personality+'/'# cohesive finetuned T5\n",
    "    # model_name = 't5-large' # \n",
    "\n",
    "    df_data = pd.read_csv(data_path + personality + '_whole.tsv', sep='\\t')\n",
    "    df = df_data[['utterance', 'labels']]\n",
    "    \n",
    "            \n",
    "    template_generate_model, template_generate_tokenizer, template_generate_model_config, template_tokenizer_wrapper = \\\n",
    "                        load_plm('t5', model_name)\n",
    "\n",
    "    ## how to select label words\n",
    "    with open('label_words/'+personality+'_words.txt', 'r') as f:\n",
    "        pos_words = f.readline().split(',')\n",
    "        neg_words = f.readline().split(',')\n",
    "\n",
    "    with open('label_words/'+personality+'_weights.txt', 'r') as f:\n",
    "        pos_weights = eval(f.readline())\n",
    "        neg_weights = eval(f.readline())\n",
    "\n",
    "    pos_dict = {}\n",
    "    for word, weight in zip(pos_words, pos_weights):\n",
    "        pos_dict[word] = weight\n",
    "\n",
    "    neg_dict = {}\n",
    "    for word, weight in zip(neg_words, neg_weights):\n",
    "        neg_dict[word] = weight\n",
    "\n",
    "    pos = sorted(pos_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)[:n]\n",
    "    neg = sorted(neg_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)[:n]\n",
    "    # print('Top', n, 'positive label words are:', pos)\n",
    "    # print('Top', n, 'negative label words are:', neg)\n",
    "\n",
    "\n",
    "\n",
    "    classes = [0,1]        \n",
    "    verbalizer = ManualVerbalizer(\n",
    "        classes = classes,\n",
    "        label_words = {\n",
    "            0 : [i[0] for i in neg] + [i[0] for i in pos], \n",
    "            1 : [i[0] for i in pos] + [i[0] for i in neg]\n",
    "        },\n",
    "        tokenizer=template_generate_tokenizer)\n",
    "\n",
    "    template = LMBFFTemplateGenerationTemplate(tokenizer = template_generate_tokenizer, \n",
    "                                               verbalizer = verbalizer, \n",
    "                                               text = '{\"placeholder\":\"text_a\"} {\"mask\"} {\"meta\":\"labelword\"} {\"mask\"}.')\n",
    "\n",
    "    \n",
    "\n",
    "    # template generation\n",
    "    template_generate_model = template_generate_model.cuda()\n",
    "\n",
    "\n",
    "    template_generator = T5TemplateGenerator(template_generate_model, \n",
    "                                             template_generate_tokenizer, \n",
    "                                             template_tokenizer_wrapper, \n",
    "                                             verbalizer, \n",
    "                                             beam_width=beam_width) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## load data\n",
    "\n",
    "    data_train = DF_Processor().get_examples(df)\n",
    "\n",
    "\n",
    "    dataloader = PromptDataLoader(data_train, \n",
    "                                  template, \n",
    "                                  tokenizer=template_generate_tokenizer, \n",
    "                                  tokenizer_wrapper_class=template_tokenizer_wrapper, \n",
    "                                  batch_size=len(data_train), \n",
    "                                  decoder_max_length=32, \n",
    "                                  max_seq_length=64, \n",
    "                                  shuffle=False, \n",
    "                                  teacher_forcing=False) # register all data at once\n",
    "\n",
    "\n",
    "    for data in dataloader:\n",
    "        data = data.cuda()\n",
    "\n",
    "        template_generator._register_buffer(data)\n",
    "\n",
    "    template_texts = template_generator._get_templates()\n",
    "    _template_texts = template_post_process(template_texts)\n",
    "\n",
    "    original_template = template.text\n",
    "\n",
    "    template_texts = []\n",
    "    for template_text in _template_texts:\n",
    "        try:\n",
    "            tmp = template_generator.convert_template(template_text, original_template)\n",
    "            template_texts.append(tmp)\n",
    "        except:\n",
    "            print(template_text)\n",
    "\n",
    "    # template_generator._show_template()\n",
    "    template_generator.release_memory()\n",
    "    # generate a number of candidate template text\n",
    "    with open('templates/'+model_name[:-1]+'_'+dataset+'_'+personality+'_templates_top_'+str(beam_width)+'.txt', 'w') as f:\n",
    "        for tmp in template_texts:\n",
    "            f.write(tmp+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wen",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
