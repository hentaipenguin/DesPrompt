{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9706563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "## self-modified\n",
    "from pipeline_base_modified import PromptDataLoader, PromptForClassification\n",
    "from template_generation_wzy import LMBFFTemplateGenerationTemplate, T5TemplateGenerator\n",
    "from openprompt.plms import load_plm\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.trainer import ClassificationRunner\n",
    "import copy\n",
    "import torch\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.plms import load_plm\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from openprompt.prompts import ManualTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f039c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_list(a,b):\n",
    "    res = []\n",
    "    if len(a) >= len(b):\n",
    "        for i in range(len(b)):\n",
    "            res.append(a[i]+b[i])# \n",
    "        for i in range(len(a) - len(b)):\n",
    "            res.append(a[len(b)+i])\n",
    "        return res\n",
    "    elif len(a) < len(b):\n",
    "        return sum_list(b, a)\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "use_cuda = True\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d633d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 2534.32it/s]\n",
      "tokenizing: 1it [00:00, 2790.62it/s]\n",
      "tokenizing: 1it [00:00, 2395.38it/s]\n",
      "tokenizing: 1it [00:00, 2334.06it/s]\n",
      "tokenizing: 1it [00:00, 2403.61it/s]\n",
      "tokenizing: 1it [00:00, 2439.97it/s]\n",
      "tokenizing: 1it [00:00, 2366.99it/s]\n",
      "tokenizing: 1it [00:00, 2465.79it/s]\n",
      "tokenizing: 1it [00:00, 1827.58it/s]\n",
      "tokenizing: 1it [00:00, 2437.13it/s]\n",
      "tokenizing: 1it [00:00, 2418.86it/s]\n",
      "tokenizing: 1it [00:00, 2332.76it/s]\n",
      "tokenizing: 1it [00:00, 2371.00it/s]\n",
      "tokenizing: 1it [00:00, 2376.38it/s]\n",
      "tokenizing: 1it [00:00, 1524.09it/s]\n",
      "tokenizing: 1it [00:00, 2326.29it/s]\n",
      "tokenizing: 1it [00:00, 2231.01it/s]\n",
      "tokenizing: 1it [00:00, 2420.26it/s]\n",
      "tokenizing: 1it [00:00, 1762.31it/s]\n",
      "tokenizing: 1it [00:00, 2385.84it/s]\n",
      "tokenizing: 1it [00:00, 2420.26it/s]\n",
      "tokenizing: 1it [00:00, 2432.89it/s]\n",
      "tokenizing: 1it [00:00, 2240.55it/s]\n",
      "tokenizing: 1it [00:00, 2267.19it/s]\n",
      "tokenizing: 1it [00:00, 1510.37it/s]\n",
      "tokenizing: 1it [00:00, 2290.72it/s]\n",
      "tokenizing: 1it [00:00, 1646.76it/s]\n",
      "tokenizing: 1it [00:00, 2451.38it/s]\n",
      "tokenizing: 1it [00:00, 2373.69it/s]\n",
      "tokenizing: 1it [00:00, 1724.63it/s]\n",
      "tokenizing: 1it [00:00, 2403.61it/s]\n",
      "tokenizing: 1it [00:00, 2398.12it/s]\n",
      "tokenizing: 1it [00:00, 2414.68it/s]\n",
      "tokenizing: 1it [00:00, 1743.99it/s]\n",
      "tokenizing: 1it [00:00, 2339.27it/s]\n",
      "tokenizing: 1it [00:00, 1558.06it/s]\n",
      "tokenizing: 1it [00:00, 2372.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 positive label words are: ['good-humoured', 'onefold', 'likeable', 'nice', 'hot', 'sadistic', 'kind', 'freehearted', 'sympathetic', 'likable', 'congenial', 'easy', 'derivative', 'fine', 'warmly', 'warm', 'easy-going', 'simpleton', 'simple-minded', 'simple', 'lukewarm', 'great', 'polite', 'courteous', 'civil', 'gracious', 'polished', 'gallant', 'affable', 'helpful', 'prepared', 'heat', 'genteel', 'baking', 'warmth', 'aflutter', 'sincere', 'warm hearted', 'solemn', 'tisane', 'sentimental', 'liqueur', 'large-minded', 'heartfelt', 'elixir', 'cordial', 'cheer', 'invigorating', 'enjoyable', 'serious-minded', 'patient of', 'nesh', 'sensitive', 'medium', 'sensible', 'aware', 'tolerant', 'caring', 'beneficial', 'charitable', 'agreeable', 'resistant', 'welfare', 'humanitarian', 'eleemosynary', 'merciful', 'conformable', 'pleasant', 'happy', 'smooth', 'hard-nosed', 'pleasing', 'free-and-easy', 'patient', 'peaceful', 'patriotic', 'affected role', 'loyal', 'trustful', 'womanly', 'sweet', 'feminine', 'thoughtful', 'pensive', 'peaceable', 'tranquil', 'generous', 'loveable', 'complaisant', 'obliging', 'well-mannered', 'well bred', 'well behaved', 'pollyannaish', 'cheerful', 'on time', 'amiable', 'lovable', 'ebullient', 'reasonable']\n",
      "Top 100 negative label words are: ['truehearted', 'cool', 'bad-mannered', 'sad', 'bad-tempered', 'hardline', 'important', 'coldness', 'cold', 'bigoted', 'free', 'parasympathetic', 'hard-and-fast', 'cute', 'complicated', 'arctic', 'common cold', 'rude', 'harsh', 'cruel', 'inimical', 'rigorous', 'unprepared', 'chilled', 'bounderish', 'insensate', 'moth-eaten', 'frigid', 'uncivil', 'brusque', 'selfish', 'rough in', 'rough', 'unsympathetic', 'unkindly', 'unsentimental', 'counteractive', 'antagonistic', 'antipathetical', 'abusive', 'unpleasant', 'reproachful', 'disagreeable', 'opprobrious', 'stoic', 'nonsensitive', 'crude', 'dangerous', 'pugnacious', 'jolting', 'rocky', 'gravelly', 'roughly', 'approximative', 'broken', 'uncut', 'warring', 'calloused', 'callous', 'war-ridden', 'roughshod', 'self-conceited', 'violent', 'agent', 'narcissistic', 'macrophobic', 'disloyal', 'inconsiderate', 'unconsidered', 'battleful', 'late', 'egotistical', 'nonpeaceful', 'thick', 'disputatious', 'combative', 'agonistical', 'insensitive', 'blunt', 'restive', 'ruthless', 'pachydermatous', 'disconnected', 'heartless', 'abrupt', 'unrefined', 'ungentle', 'bearish', 'combatant', 'tough', 'coarse', 'unpolished', 'impolite', 'domineering', 'uncourteous', 'remorseless', 'precipitous', 'unmannerly', 'discourteous', 'belligerent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 2737.80it/s]\n",
      "tokenizing: 1it [00:00, 2455.68it/s]\n",
      "tokenizing: 1it [00:00, 2914.74it/s]\n",
      "tokenizing: 1it [00:00, 2439.97it/s]\n",
      "tokenizing: 1it [00:00, 2400.86it/s]\n",
      "tokenizing: 1it [00:00, 2413.29it/s]\n",
      "tokenizing: 1it [00:00, 2366.99it/s]\n",
      "tokenizing: 1it [00:00, 2857.16it/s]\n",
      "tokenizing: 1it [00:00, 1633.93it/s]\n",
      "tokenizing: 1it [00:00, 2379.07it/s]\n",
      "tokenizing: 1it [00:00, 2404.99it/s]\n",
      "tokenizing: 1it [00:00, 2473.06it/s]\n",
      "tokenizing: 1it [00:00, 2384.48it/s]\n",
      "tokenizing: 1it [00:00, 2406.37it/s]\n",
      "tokenizing: 1it [00:00, 2444.23it/s]\n",
      "tokenizing: 1it [00:00, 2272.10it/s]\n",
      "tokenizing: 1it [00:00, 1541.46it/s]\n",
      "tokenizing: 1it [00:00, 2273.34it/s]\n",
      "tokenizing: 1it [00:00, 1691.93it/s]\n",
      "tokenizing: 1it [00:00, 2377.72it/s]\n",
      "tokenizing: 1it [00:00, 2308.37it/s]\n",
      "tokenizing: 1it [00:00, 2391.28it/s]\n",
      "tokenizing: 1it [00:00, 2299.51it/s]\n",
      "tokenizing: 1it [00:00, 2275.80it/s]\n",
      "tokenizing: 1it [00:00, 1572.08it/s]\n",
      "tokenizing: 1it [00:00, 2322.43it/s]\n",
      "tokenizing: 1it [00:00, 2359.00it/s]\n",
      "tokenizing: 1it [00:00, 2310.91it/s]\n",
      "tokenizing: 1it [00:00, 2332.76it/s]\n",
      "tokenizing: 1it [00:00, 2347.12it/s]\n",
      "tokenizing: 1it [00:00, 2399.49it/s]\n",
      "tokenizing: 1it [00:00, 2398.12it/s]\n",
      "tokenizing: 1it [00:00, 2442.81it/s]\n",
      "tokenizing: 1it [00:00, 2373.69it/s]\n",
      "tokenizing: 1it [00:00, 2355.03it/s]\n",
      "tokenizing: 1it [00:00, 2360.33it/s]\n",
      "tokenizing: 1it [00:00, 2348.43it/s]\n",
      "tokenizing: 1it [00:00, 2331.46it/s]\n",
      "tokenizing: 1it [00:00, 1698.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 positive label words are: ['truehearted', 'sadistic', 'organized', 'organised', 'accurate', 'on time', 'sad', 'precise', 'exact', 'sweetheart', 'hard work', 'hard', 'hard-and-fast', 'hardworking', 'hard working', 'right winger', 'fastidious', 'hard-nosed', 'authentic', 'secure', 'reliable', 'canny', 'exacting', 'logical', 'brief', 'straightaway', 'acquisitive', 'consistent', 'measured', 'concerned', 'legitimate', 'careful', 'succinct', 'concise', 'prompt', 'coherent', 'attentive', 'command prompt', 'anxious', 'lucid', 'mindful', 'terse', 'motivate', 'solicitous', 'work', 'strong', 'positive', 'regular', 'reproducible', 'steady', 'meticulous', 'painstaking', 'steadily', 'brace', 'punctilious', 'unfluctuating', 'unfaltering', 'decided', 'decisive', 'conclusive', 'reckful', 'simpleton', 'simple-minded', 'simple', 'difficult', 'tauten', 'free', 'timely', 'solid', 'productive', 'responsible for', 'responsible', 'company', 'firm', 'enterprise', 'punctual', 'corporation', 'unwaveringly', 'immobile', 'stable', 'busy', 'creditworthy', 'economic', 'cautious', 'economical', 'effective', 'stinting', 'limited', 'self-respectful', 'noetic', 'tense', 'thoroughgoing', 'thorough', 'focused', 'strict', 'determined', 'purposeful', 'nonindulgent', 'unyielding', 'unadulterated']\n",
      "Top 100 negative label words are: ['good-humoured', 'disorganized', 'inexact', 'hit-or-miss', 'bad-mannered', 'haphazardly', 'haphazard', 'disorderly', 'happy-go-lucky', 'higgledy-piggledy', 'late', 'young', 'head-in-the-clouds', 'scatterbrained', 'waterlogged', 'illogical', 'neglected', 'atypical', 'loose-fitting', 'sloppy', 'slapdash', 'slipshod', 'slovenly', 'unsound', 'overemotional', 'unreasoning', 'immature', 'green', 'contradictory', 'incompatible', 'undependable', 'cool', 'stupid', 'bad-tempered', 'unfledged', 'unsteady', 'reckless', 'negligent', 'erratic', 'dropstone', 'quicksilver', 'planetary', 'lazy', 'aflutter', 'irresponsible', 'indecisive', 'faineant', 'unretentive', 'unmindful', 'forgetful', 'bone idle', 'free-and-easy', 'areligious', 'easy-going', 'unstable', 'mentally ill', 'childish', 'neglecting', 'precarious', 'incautious', 'uncautious', 'labile', 'instable', 'permissive', 'daring', 'absurd', 'directionless', 'undignified', 'foolhardy', 'happy', 'aimless', 'superficial', 'careless', 'easy', 'capricious', 'regardless', 'impulsive', 'unprompted', 'loose', 'slack', 'lax', 'bow', 'irrational', 'arational', 'chaotic', 'inefficient', 'theoretical', 'ineffective', 'unsystematic', 'spendthrift', 'liberal', 'impractical', 'wishy-washy', 'disrespectful', 'wasteful', 'overindulgent', 'unconventional', 'uneconomical', 'moving', 'inconsistent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 2294.48it/s]\n",
      "tokenizing: 1it [00:00, 2427.26it/s]\n",
      "tokenizing: 1it [00:00, 2371.00it/s]\n",
      "tokenizing: 1it [00:00, 2389.92it/s]\n",
      "tokenizing: 1it [00:00, 2360.33it/s]\n",
      "tokenizing: 1it [00:00, 2351.07it/s]\n",
      "tokenizing: 1it [00:00, 2357.68it/s]\n",
      "tokenizing: 1it [00:00, 2347.12it/s]\n",
      "tokenizing: 1it [00:00, 2303.30it/s]\n",
      "tokenizing: 1it [00:00, 1741.10it/s]\n",
      "tokenizing: 1it [00:00, 2423.05it/s]\n",
      "tokenizing: 1it [00:00, 2417.47it/s]\n",
      "tokenizing: 1it [00:00, 2341.88it/s]\n",
      "tokenizing: 1it [00:00, 2353.71it/s]\n",
      "tokenizing: 1it [00:00, 1373.83it/s]\n",
      "tokenizing: 1it [00:00, 2377.72it/s]\n",
      "tokenizing: 1it [00:00, 2239.35it/s]\n",
      "tokenizing: 1it [00:00, 2290.72it/s]\n",
      "tokenizing: 1it [00:00, 1611.95it/s]\n",
      "tokenizing: 1it [00:00, 2437.13it/s]\n",
      "tokenizing: 1it [00:00, 2369.66it/s]\n",
      "tokenizing: 1it [00:00, 2348.43it/s]\n",
      "tokenizing: 1it [00:00, 2331.46it/s]\n",
      "tokenizing: 1it [00:00, 2259.86it/s]\n",
      "tokenizing: 1it [00:00, 2304.56it/s]\n",
      "tokenizing: 1it [00:00, 2216.86it/s]\n",
      "tokenizing: 1it [00:00, 2431.48it/s]\n",
      "tokenizing: 1it [00:00, 2383.13it/s]\n",
      "tokenizing: 1it [00:00, 1796.28it/s]\n",
      "tokenizing: 1it [00:00, 2455.68it/s]\n",
      "tokenizing: 1it [00:00, 2424.45it/s]\n",
      "tokenizing: 1it [00:00, 2427.26it/s]\n",
      "tokenizing: 1it [00:00, 2399.49it/s]\n",
      "tokenizing: 1it [00:00, 1516.38it/s]\n",
      "tokenizing: 1it [00:00, 2277.04it/s]\n",
      "tokenizing: 1it [00:00, 2222.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 positive label words are: ['happy', 'happy-go-lucky', 'good-humoured', 'truehearted', 'sadistic', 'long winded', 'bigmouthed', 'loud', 'brazen', 'extraverted', 'noisy', 'strong-growing', 'funny', 'noise', 'extravertive', 'vocal', 'sounded', 'shout', 'extroverted', 'extrospective', 'extrorse', 'talkative', 'drivellous', 'chatty', 'verbose', 'garrulous', 'loquacious', 'prolix', 'aggressive', 'dauntless', 'bellicose', 'daredevil', 'bad-mannered', 'sociable', 'exciting', 'social', 'going strong', 'up-and-coming', 'nice', 'likeable', 'boldface', 'bold', 'non finite verb', 'verbal', 'bluff', 'audacious', 'assertive', 'lectic', 'sicker', 'enthusiastic', 'self assured', 'confident', 'sure-footed', 'hot', 'attractive', 'mettlesome', 'enlivened', 'spirited', 'bouncing', 'communicative', 'hard work', 'hard', 'appealing', 'simple', 'energetic', 'charismatic', 'magnetic', 'magnetised', 'free', 'gay', 'cute', 'easy-going', 'shiny', 'boldness', 'rambunctious', 'daring', 'straight-from-the-shoulder', 'dare', 'venturesome', 'content', 'outspoken', 'alluring', 'tasty', 'dominant allele', 'dominant', 'joyful', 'prevalent', 'pleased', 'spicy', 'mirthful', 'exultant', 'vivacious', 'ecstatic', 'merry', 'festive', 'imposing', 'predominant', 'rattling', 'active voice', 'active agent']\n",
      "Top 100 negative label words are: ['sad', 'cool', 'dumb', 'quieten', 'quiet down', 'quiet', 'bad-tempered', 'aflutter', 'silent', 'reclusive', 'withdrawn', 'soundless', 'unsounded', 'tacit', 'calm', 'introverted', 'shy', 'diffident', 'unruffled', 'silence', 'repose', 'quietly', 'lull', 'hushed', 'tranquillity', 'dour', 'laconic', 'mute', 'meek', 'reserved', 'faint-hearted', 'timid', 'timorous', 'asocial', 'bashful', 'unsocial', 'antisocial', 'about', 'inimical', 'inexpressive', 'deadening', 'inhibited', 'simpleton', 'simple-minded', 'simple', 'implied', 'unsaid', 'coldness', 'cold', 'closemouthed', 'right winger', 'safe', 'passive voice', 'passive', 'non magnetic', 'uninvolved', 'idle', 'repulsive', 'geographic', 'disinterested', 'antimagnetic', 'dullen', 'dull', 'guarded', 'restrained', 'reticent', 'dense', 'thudding', 'leaden', 'muffle', 'numb', 'lackluster', 'softened', 'pall', 'miserable', 'unhappy', 'bland', 'obedient', 'suave', 'insipid', 'flavourless', 'inactive', 'complicated', 'sombre', 'sober', 'easy', 'melancholy', 'composed', 'staid', 'sedate', 'somber', 'uncongenial', 'unfriendly', 'melancholic', 'depressed', 'hostile', 'unsociable', 'on time', 'gentle', 'unadventurous']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 2882.68it/s]\n",
      "tokenizing: 1it [00:00, 2418.86it/s]\n",
      "tokenizing: 1it [00:00, 2380.42it/s]\n",
      "tokenizing: 1it [00:00, 2403.61it/s]\n",
      "tokenizing: 1it [00:00, 2395.38it/s]\n",
      "tokenizing: 1it [00:00, 2400.86it/s]\n",
      "tokenizing: 1it [00:00, 2355.03it/s]\n",
      "tokenizing: 1it [00:00, 2348.43it/s]\n",
      "tokenizing: 1it [00:00, 2339.27it/s]\n",
      "tokenizing: 1it [00:00, 2399.49it/s]\n",
      "tokenizing: 1it [00:00, 2430.07it/s]\n",
      "tokenizing: 1it [00:00, 2447.09it/s]\n",
      "tokenizing: 1it [00:00, 2387.20it/s]\n",
      "tokenizing: 1it [00:00, 2416.07it/s]\n",
      "tokenizing: 1it [00:00, 1576.21it/s]\n",
      "tokenizing: 1it [00:00, 1464.49it/s]\n",
      "tokenizing: 1it [00:00, 2269.65it/s]\n",
      "tokenizing: 1it [00:00, 1529.09it/s]\n",
      "tokenizing: 1it [00:00, 2232.20it/s]\n",
      "tokenizing: 1it [00:00, 2407.75it/s]\n",
      "tokenizing: 1it [00:00, 2389.92it/s]\n",
      "tokenizing: 1it [00:00, 2437.13it/s]\n",
      "tokenizing: 1it [00:00, 2290.72it/s]\n",
      "tokenizing: 1it [00:00, 2332.76it/s]\n",
      "tokenizing: 1it [00:00, 2331.46it/s]\n",
      "tokenizing: 1it [00:00, 2323.71it/s]\n",
      "tokenizing: 1it [00:00, 1559.80it/s]\n",
      "tokenizing: 1it [00:00, 1548.86it/s]\n",
      "tokenizing: 1it [00:00, 1565.04it/s]\n",
      "tokenizing: 1it [00:00, 2400.86it/s]\n",
      "tokenizing: 1it [00:00, 2373.69it/s]\n",
      "tokenizing: 1it [00:00, 2417.47it/s]\n",
      "tokenizing: 1it [00:00, 2335.36it/s]\n",
      "tokenizing: 1it [00:00, 2396.75it/s]\n",
      "tokenizing: 1it [00:00, 2352.39it/s]\n",
      "tokenizing: 1it [00:00, 2375.03it/s]\n",
      "tokenizing: 1it [00:00, 2362.99it/s]\n",
      "tokenizing: 1it [00:00, 1515.28it/s]\n",
      "tokenizing: 1it [00:00, 1546.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 positive label words are: ['good-humoured', 'complicated', 'great', 'sad', 'sadistic', 'truehearted', 'well-informed', 'free', 'intelligent', 'level-headed', 'silly', 'hardworking', 'hard working', 'smart', 'funny', 'atypical', 'complex', 'likeable', 'intellectual', 'cerebral', 'voguish', 'freehearted', 'ache', 'building complex', 'bad-mannered', 'philosophical', 'philosophic', 'brilliant', 'deep', 'subtle', 'selfish', 'bright', 'promising', 'low', 'heavy', 'innovative', 'forward-looking', 'adroit', 'artful', 'unconventional', 'ready', 'undimmed', 'burnished', 'lustrous', 'fulgent', 'propitious', 'glimmering', 'brightly', 'luminiferous', 'innovatory', 'innovational', 'gifted', 'ingenious', 'deeply', 'able', 'fertile', 'knowledgeable', 'bass', 'inscrutable', 'agog', 'trench', 'recondite', 'sagacious', 'inquisitive', 'versed', 'insightful', 'nonconforming', 'analytical', 'analytic', 'perspicacious', 'introspective', 'important', 'meditative', 'contemplative', 'perceptive', 'articulate', 'give voice', 'joint', 'pronounce', 'articulated', 'original', 'simple', 'inventive', 'untraditional', 'nontraditional', 'quick witted', 'restive', 'clever', 'noetic', 'individualistic', 'creative', 'sharp witted', 'hard-nosed', 'intense', 'apt', 'calculating', 'astute', 'shrewd', 'alluring', 'rational number']\n",
      "Top 100 negative label words are: ['stupid', 'simpleton', 'simple-minded', 'simple', 'happy-go-lucky', 'nice', 'onefold', 'nonintellectual', 'backward', 'basic', 'garish', 'dark', 'nonphilosophical', 'ineffectual', 'really dark', 'really', 'innocent', 'right winger', 'plain', 'round-eyed', 'unsuspecting', 'uncomplicated', 'high', 'super', 'dim dark', 'dim', 'light', 'selfish', 'bad-tempered', 'narrow', 'happy', 'unknowledgeable', 'cool', 'inarticulate', 'typical', 'hardline', 'conventional', 'ceremonious', 'hard-and-fast', 'traditionary', 'traditional', 'about', 'inexpressive', 'absurd', 'uninquisitive', 'uninquiring', 'childish', 'aflutter', 'unintelligent', 'irrational', 'clumsy', 'imitative', 'annihilative', 'otherworldly', 'arational', 'mean', 'grandiloquent', 'shallow', 'surly', 'pompous', 'ignorant', 'needy', 'contingent', 'enslaved', 'foolish', 'nasty', 'dependent', 'drug-addicted', 'dependant', 'shoal', 'pendent', 'tory', 'blue', 'conservative', 'vengeful', 'reactionary', 'traditionalist', 'buttoned-down', 'revengeful', 'materialistic', 'unwise', 'non progressive', 'despiteful', 'regressive', 'spiteful', 'vindictive', 'meek', 'unversed', 'unlettered', 'patronizing', 'uninteltectual', 'poseur', 'ostentatious', 'pretentious', 'predictable', 'unscrupulous', 'clerical', 'biddable', 'naive', 'moralistic']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1it [00:00, 2449.94it/s]\n",
      "tokenizing: 1it [00:00, 2939.25it/s]\n",
      "tokenizing: 1it [00:00, 2467.24it/s]\n",
      "tokenizing: 1it [00:00, 2451.38it/s]\n",
      "tokenizing: 1it [00:00, 2366.99it/s]\n",
      "tokenizing: 1it [00:00, 2304.56it/s]\n",
      "tokenizing: 1it [00:00, 2325.00it/s]\n",
      "tokenizing: 1it [00:00, 1527.98it/s]\n",
      "tokenizing: 1it [00:00, 2477.44it/s]\n",
      "tokenizing: 1it [00:00, 2418.86it/s]\n",
      "tokenizing: 1it [00:00, 2444.23it/s]\n",
      "tokenizing: 1it [00:00, 2409.13it/s]\n",
      "tokenizing: 1it [00:00, 2416.07it/s]\n",
      "tokenizing: 1it [00:00, 2296.99it/s]\n",
      "tokenizing: 1it [00:00, 2285.72it/s]\n",
      "tokenizing: 1it [00:00, 2348.43it/s]\n",
      "tokenizing: 1it [00:00, 2316.02it/s]\n",
      "tokenizing: 1it [00:00, 2289.47it/s]\n",
      "tokenizing: 1it [00:00, 2383.13it/s]\n",
      "tokenizing: 1it [00:00, 2373.69it/s]\n",
      "tokenizing: 1it [00:00, 2418.86it/s]\n",
      "tokenizing: 1it [00:00, 2322.43it/s]\n",
      "tokenizing: 1it [00:00, 2328.88it/s]\n",
      "tokenizing: 1it [00:00, 2308.37it/s]\n",
      "tokenizing: 1it [00:00, 2291.97it/s]\n",
      "tokenizing: 1it [00:00, 2259.86it/s]\n",
      "tokenizing: 1it [00:00, 2273.34it/s]\n",
      "tokenizing: 1it [00:00, 2122.62it/s]\n",
      "tokenizing: 1it [00:00, 1831.57it/s]\n",
      "tokenizing: 1it [00:00, 2416.07it/s]\n",
      "tokenizing: 1it [00:00, 1358.26it/s]\n",
      "tokenizing: 1it [00:00, 2398.12it/s]\n",
      "tokenizing: 1it [00:00, 2285.72it/s]\n",
      "tokenizing: 1it [00:00, 2395.38it/s]\n",
      "tokenizing: 1it [00:00, 2407.75it/s]\n",
      "tokenizing: 1it [00:00, 2373.69it/s]\n",
      "tokenizing: 1it [00:00, 1495.30it/s]\n",
      "tokenizing: 1it [00:00, 1435.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 positive label words are: ['aflutter', 'bad-tempered', 'sad', 'overjealous', 'important', 'touchy', 'moody', 'Helen Newington Wills', 'glowering', 'Dwight Lyman Moody', 'easy', 'thin-skinned', 'sadistic', 'ticklish', 'temperamental', 'emotional', 'worked up', 'nettlesome', 'effusive', 'irritable', 'stressed', 'jealous', 'green with envy', 'justificatory', 'ineffectual', 'envious', 'covetous', 'overcautious', 'possessive', 'genitive', 'raring', 'fidgety', 'fretful', 'querulous', 'impatient', 'selfish', 'agent', 'macrophobic', 'self-pitying', 'nervous', 'female', 'emasculated', 'high-strung', 'ratty', 'crabby', 'queasy', 'skittish', 'defensive', 'truculent', 'cantankerous', 'grumpy', 'surly', 'grouchy', 'insecure', 'frightening', 'unsafe', 'cranky', 'dangerous', 'faultfinding', 'anxious', 'frightened', 'unquiet', 'hardline', 'disturbed', 'fearful', 'uneasy', 'quarrelsome', 'captious', 'frightful', 'angstful', 'womanly', 'feminine', 'aware', 'afraid', 'nesh', 'about', 'stupid', 'sensitive', 'medium', 'sensible', 'caring', 'finicky', 'finical', 'coward', 'high-and-mighty', 'contradictory', 'sentimental', 'incompatible', 'on time', 'violent', 'warring', 'cliquish', 'clannish', 'snobby', 'snobbish', 'unsteady', 'complicated', 'bossy', 'significant', 'prying']\n",
      "Top 100 negative label words are: ['good-humoured', 'truehearted', 'easy-going', 'happy-go-lucky', 'simpleton', 'simple-minded', 'simple', 'free-and-easy', 'happy', 'relaxed', 'unemotional', 'dispassionate', 'free', 'patient', 'affected role', 'manly', 'male', 'masculine', 'offensive', 'undemanding', 'unexd table', 'sicker', 'self confident', 'sweetheart', 'large-minded', 'patient of', 'consistent', 'tolerant', 'resistant', 'braw', 'freehearted', 'brave', 'weather', 'nice', 'courageous', 'stoic', 'nonsensitive', 'unsentimental', 'reproducible', 'peaceful', 'steady', 'regular', 'unofficial', 'sure-footed', 'informal', 'steadily', 'brace', 'colloquial', 'unfluctuating', 'unfaltering', 'self assured', 'confident', 'inexpressive', 'peaceable', 'tranquil', 'sweet', 'assured', 'uncritical', 'likeable', 'inarticulate', 'down-to-earth', 'lead', 'insensitive', 'stable', 'hard-nosed', 'noetic', 'full-blooded', 'lazy', 'passionless', 'emotionless', 'clear', 'nongreedy', 'unsensitive', 'numbed', 'hardworking', 'hard working', 'earthy', 'reasonable', 'nonchalant', 'unpretentious', 'unassuming', 'blasé', 'tough', 'reasonous', 'perfunctory', 'occasional', 'accidental', 'problematical', 'ruffianly', 'casual', 'everyday', 'fooling', 'fortuitous', 'incidental', 'contingent', 'satisfying', 'onefold', 'rational number', 'rational', 'cool']\n"
     ]
    }
   ],
   "source": [
    "Personalities = ['A','C','E','O','N']\n",
    "\n",
    "for personality in Personalities:\n",
    "    with open('label_words/' + personality + '_words.txt', 'r') as f:\n",
    "        pos = f.readline().split(',')\n",
    "        neg = f.readline().split(',')\n",
    "\n",
    "    with open('label_words/' + personality + '_weights.txt', 'r') as f:\n",
    "        pos_weights = eval(f.readline())\n",
    "        neg_weights = eval(f.readline())\n",
    "    \n",
    "    \n",
    "    candidate_templates = []\n",
    "    datasets = ['Friends', 'MyPersonality', 'Essay', 'Pan']\n",
    "    for dataset in datasets:\n",
    "        with open('templates/Adapted_t5_large_'+dataset+'_'+personality+'_templates_top_10.txt', 'r') as f:\n",
    "            candidate_templates += [i.strip() for i in f.readlines()]\n",
    "    \n",
    "    classes = [0,1]        \n",
    "    myverbalizer = ManualVerbalizer(\n",
    "        classes = classes,\n",
    "        label_words = {\n",
    "            0 : neg, \n",
    "            1 : pos\n",
    "        },\n",
    "        tokenizer=tokenizer)\n",
    "\n",
    "    from openprompt import PromptDataLoader\n",
    "    batch_size = 1\n",
    "\n",
    "    overall_logits = []\n",
    "\n",
    "    for i in range(len(candidate_templates)):\n",
    "        sample = InputExample(guid=i, text_a=\"\", label=0)\n",
    "        mytemplate = ManualTemplate(\n",
    "            text = candidate_templates[i],\n",
    "            tokenizer = tokenizer,\n",
    "        )\n",
    "        prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer, freeze_plm=True)\n",
    "        if use_cuda:\n",
    "            prompt_model = prompt_model.cuda()\n",
    "        prompt_model.eval()\n",
    "        validation_dataloader = PromptDataLoader(dataset=[sample], template=mytemplate, tokenizer=tokenizer,\n",
    "            tokenizer_wrapper_class=WrapperClass, max_seq_length=128,\n",
    "            batch_size=batch_size, shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "            truncate_method=\"head\")\n",
    "\n",
    "        for step, inputs in enumerate(validation_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model.forward_without_verbalize(inputs)\n",
    "            label_words_logits = prompt_model.verbalizer.project(logits)\n",
    "            label_words_probs = prompt_model.verbalizer.normalize(label_words_logits)\n",
    "            label_words_logits = label_words_probs # torch.log(label_words_probs+1e-15)\n",
    "            overall_logits.append(label_words_logits.detach())\n",
    "\n",
    "    overall_logits = torch.cat(overall_logits)\n",
    "    overall_logits = torch.mean(overall_logits, 0)\n",
    "\n",
    "    neg_logits = [0.1*float(i) for i in list(overall_logits[0]/overall_logits[0].sum().cpu())]\n",
    "    pos_logits = [0.1*float(i) for i in list(overall_logits[1]/overall_logits[1].sum().cpu())]\n",
    "\n",
    "    pos_weights_ = [i/sum(pos_weights) for i in pos_weights]\n",
    "    neg_weights_ = [i/sum(neg_weights) for i in neg_weights]\n",
    "\n",
    "\n",
    "    pos_logit_weight = sum_list(pos_weights_, pos_logits)\n",
    "    neg_logit_weight = sum_list(neg_weights_, neg_logits)\n",
    "\n",
    "\n",
    "\n",
    "    pos_dict = {}\n",
    "    for word, weight in zip(pos, pos_logit_weight[:len(pos)]):\n",
    "        pos_dict[word] = weight\n",
    "\n",
    "    neg_dict = {}\n",
    "    for word, weight in zip(neg, neg_logit_weight[:len(neg)]):\n",
    "        neg_dict[word] = weight  \n",
    "\n",
    "    n = 100\n",
    "    pos_ = sorted(pos_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)[:n]\n",
    "    neg_ = sorted(neg_dict.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)[:n]\n",
    "    \n",
    "    print('Top', n, 'positive label words are:', [i[0] for i in pos_])\n",
    "    print('Top', n, 'negative label words are:', [i[0] for i in neg_])\n",
    "\n",
    "    with open('label_words/posterior_'+personality+'_label_words.txt', 'w') as f:\n",
    "        f.write(','.join([i[0] for i in pos_]))\n",
    "        f.write(','.join([i[0] for i in neg_]))\n",
    "    with open('label_words/posterior_'+personality+'_label_weights.txt', 'w') as f:\n",
    "        f.write(str([i[1] for i in pos_]))\n",
    "        f.write('\\n')\n",
    "        f.write(str([i[1] for i in neg_]))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wen",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
