{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9d7ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c0cf23",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c304052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5631, 1)\n",
      "(24140, 1)\n",
      "(54665, 1)\n",
      "(213084, 1)\n",
      "(200015, 1)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "def sent_tokenize(df,sent_method):\n",
    "    sents = []\n",
    "    for index,row in df.iterrows():\n",
    "        sent = []\n",
    "        tempSent = row['utterance'].split('|||')\n",
    "        if sent_method == 'nltk':\n",
    "            for i in tempSent:\n",
    "                sent = sent + nltk.sent_tokenize(i)\n",
    "        elif sent_method == 'comma':\n",
    "            for i in tempSent:\n",
    "                sent = sent + [j.strip().lower() for j in re.split(r\"[,,.,?,!,;()]\",i) if len(j.strip().split(' '))>1]\n",
    "        sents = sents + sent\n",
    "    return sents\n",
    "\n",
    "\n",
    "data_paths = [\n",
    "    '../data/FriendsPersona/Friends_A_whole.tsv',\n",
    "    '../data/myPersonality/MyPersonality_A_whole.tsv',\n",
    "    '../data/pan2015/Pan_A_whole.tsv',\n",
    "    '../data/Essay/Essay_A_whole.tsv',\n",
    "#     '../data/Kaggle_mbti/Kaggle_map_label_words_comma.tsv'  \n",
    "]\n",
    "\n",
    "df_data = pd.DataFrame([])\n",
    "for path in data_paths:\n",
    "    df = pd.read_csv(path,  sep='\\t')\n",
    "    sents = sent_tokenize(df,'comma')\n",
    "    df_sents = pd.DataFrame(sents)\n",
    "    df_data = pd.concat([df_data, df_sents], axis=0)\n",
    "    print(df_data.shape)\n",
    "df_data = df_data.drop_duplicates()\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26deafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(pos, row):\n",
    "    if pos == 1:\n",
    "        return row['sent'].split(row['label_word'])[0]\n",
    "    elif pos == 2:\n",
    "        try:\n",
    "            return row['sent'].split(row['label_word'])[1]\n",
    "        except:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbcac5",
   "metadata": {},
   "source": [
    "## Construct Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75018307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_input_and_target(df):\n",
    "    span_1   = '<extra_id_0> '\n",
    "    span_2   = ' <extra_id_1> '\n",
    "    span_end = ' <extra_id_2>'\n",
    "    tmp_df = df.fillna(\" \")\n",
    "    tmp_df['inputs'] = span_1 + tmp_df['label_word'] + span_2\n",
    "    tmp_df['target'] = span_1 + tmp_df['span_1'] + span_2 + tmp_df['span_2'] + span_end\n",
    "    return list(tmp_df['inputs']), list(tmp_df['target'])\n",
    "\n",
    "\n",
    "\n",
    "def construct_training(df_data_, tokenizer):\n",
    "    inputs, targets = get_input_and_target(df_data_)\n",
    "    max_len    = 20\n",
    "    batch_size = 16\n",
    "\n",
    "    inputs  = [tokenizer.encode(sent, add_special_tokens=True, max_length=max_len, pad_to_max_length=True) for sent in inputs]\n",
    "    targets = [tokenizer.encode(sent, add_special_tokens=True, max_length=max_len, pad_to_max_length=True) for sent in targets]\n",
    "\n",
    "    input_attention_masks = [[float(i>0) for i in seq] for seq in inputs]\n",
    "\n",
    "\n",
    "\n",
    "    train_inputs  = inputs\n",
    "    train_masks   = input_attention_masks\n",
    "    train_targets = targets\n",
    "\n",
    "\n",
    "    train_inputs     = torch.tensor(train_inputs)\n",
    "    train_targets    = torch.tensor(train_targets)\n",
    "    train_masks      = torch.tensor(train_masks)\n",
    "\n",
    "    train_data       = TensorDataset(train_inputs, train_masks, train_targets)\n",
    "    train_sampler    = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a254b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trait  A ...\n",
      "Number of matched label words:  654\n",
      "Top 10 matched label words:  [('great', 1553), ('happy', 1276), ('nice', 1207), ('able', 998), ('funny', 779), ('reason', 706), ('cold', 516), ('easy', 504), ('free', 501), ('ready', 463)]\n",
      "Number of sents containing label words:  23979\n",
      "Top 10 sents containing label words:  [[\"i'm nervous\", 'nervous'], ['you look so young', 'young'], ['very nice touch', 'nice'], [\"these'll go great in my new place\", 'great'], ['based on serious stuff', 'serious'], [\"i'd be able to be a stand-up guy and go the distance\", 'able'], [\"i don't mean to be disrespectful\", 'disrespectful'], ['this is great', 'great'], ['very sexy', 'sexy'], ['joey gets her something really great', 'great']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct Training Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second phase Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_141613/4087040875.py:76: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0852de36cfa848c0b732fa009320d1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "3286.6157561540604\n",
      "<====================== Epoch 2 ======================>\n",
      "2841.5701818466187\n",
      "<====================== Epoch 3 ======================>\n",
      "2586.7103914022446\n",
      "<====================== Epoch 4 ======================>\n",
      "2365.190559208393\n",
      "<====================== Epoch 5 ======================>\n",
      "2177.101964890957\n",
      "<====================== Epoch 6 ======================>\n",
      "2012.837809085846\n",
      "<====================== Epoch 7 ======================>\n",
      "1864.817135334015\n",
      "<====================== Epoch 8 ======================>\n",
      "1731.005232334137\n",
      "<====================== Epoch 9 ======================>\n",
      "1613.3676114678383\n",
      "<====================== Epoch 10 ======================>\n",
      "1503.6499709486961\n",
      "<====================== Epoch 11 ======================>\n",
      "1407.6912552118301\n",
      "<====================== Epoch 12 ======================>\n",
      "1315.3482387065887\n",
      "<====================== Epoch 13 ======================>\n",
      "1236.8762281537056\n",
      "<====================== Epoch 14 ======================>\n",
      "1163.870855331421\n",
      "<====================== Epoch 15 ======================>\n",
      "1101.2429710924625\n",
      "<====================== Epoch 16 ======================>\n",
      "1044.7848420739174\n",
      "<====================== Epoch 17 ======================>\n",
      "992.5419355928898\n",
      "<====================== Epoch 18 ======================>\n",
      "948.064116448164\n",
      "<====================== Epoch 19 ======================>\n",
      "911.4604422152042\n",
      "<====================== Epoch 20 ======================>\n",
      "875.5463183820248\n",
      "1:48:44.164257\n",
      "\n",
      "Processing trait  C ...\n",
      "Number of matched label words:  646\n",
      "Top 10 matched label words:  [('great', 1553), ('happy', 1276), ('nice', 1207), ('able', 998), ('funny', 779), ('reason', 706), ('cold', 516), ('easy', 504), ('free', 501), ('ready', 463)]\n",
      "Number of sents containing label words:  23696\n",
      "Top 10 sents containing label words:  [[\"i'm nervous\", 'nervous'], ['you look so young', 'young'], ['very nice touch', 'nice'], [\"these'll go great in my new place\", 'great'], ['based on serious stuff', 'serious'], [\"i'd be able to be a stand-up guy and go the distance\", 'able'], [\"i don't mean to be disrespectful\", 'disrespectful'], ['this is great', 'great'], ['very sexy', 'sexy'], ['joey gets her something really great', 'great']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct Training Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second phase Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_141613/4087040875.py:76: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907d292d5f5f475694c2900e6627153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "3252.3868144750595\n",
      "<====================== Epoch 2 ======================>\n",
      "2803.1521847248077\n",
      "<====================== Epoch 3 ======================>\n",
      "2549.990755200386\n",
      "<====================== Epoch 4 ======================>\n",
      "2333.9466296434402\n",
      "<====================== Epoch 5 ======================>\n",
      "2153.1524339318275\n",
      "<====================== Epoch 6 ======================>\n",
      "1990.0079635977745\n",
      "<====================== Epoch 7 ======================>\n",
      "1847.2088550329208\n",
      "<====================== Epoch 8 ======================>\n",
      "1717.2773668169975\n",
      "<====================== Epoch 9 ======================>\n",
      "1599.8275401592255\n",
      "<====================== Epoch 10 ======================>\n",
      "1490.571525633335\n",
      "<====================== Epoch 11 ======================>\n",
      "1390.704477250576\n",
      "<====================== Epoch 12 ======================>\n",
      "1308.6300648450851\n",
      "<====================== Epoch 13 ======================>\n",
      "1226.904787659645\n",
      "<====================== Epoch 14 ======================>\n",
      "1155.464589357376\n",
      "<====================== Epoch 15 ======================>\n",
      "1095.9962752461433\n",
      "<====================== Epoch 16 ======================>\n",
      "1037.0590411424637\n",
      "<====================== Epoch 17 ======================>\n",
      "986.0407022833824\n",
      "<====================== Epoch 18 ======================>\n",
      "943.413983643055\n",
      "<====================== Epoch 19 ======================>\n",
      "902.4983675181866\n",
      "<====================== Epoch 20 ======================>\n",
      "867.0297521352768\n",
      "1:47:22.305013\n",
      "\n",
      "Processing trait  E ...\n",
      "Number of matched label words:  628\n",
      "Top 10 matched label words:  [('great', 1553), ('happy', 1276), ('nice', 1207), ('able', 998), ('funny', 779), ('reason', 706), ('cold', 516), ('easy', 504), ('free', 501), ('ready', 463)]\n",
      "Number of sents containing label words:  23375\n",
      "Top 10 sents containing label words:  [[\"i'm nervous\", 'nervous'], ['very nice touch', 'nice'], [\"these'll go great in my new place\", 'great'], ['based on serious stuff', 'serious'], [\"i'd be able to be a stand-up guy and go the distance\", 'able'], ['this is great', 'great'], ['very sexy', 'sexy'], ['joey gets her something really great', 'great'], ['very funny', 'funny'], [\"i'm cold\", 'cold']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct Training Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second phase Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_141613/4087040875.py:76: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01160dc5078f4a7393499513f5791a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "3194.416847229004\n",
      "<====================== Epoch 2 ======================>\n",
      "2757.4696955680847\n",
      "<====================== Epoch 3 ======================>\n",
      "2507.75208568573\n",
      "<====================== Epoch 4 ======================>\n",
      "2293.8839213848114\n",
      "<====================== Epoch 5 ======================>\n",
      "2113.081297814846\n",
      "<====================== Epoch 6 ======================>\n",
      "1953.9884468317032\n",
      "<====================== Epoch 7 ======================>\n",
      "1812.9346996545792\n",
      "<====================== Epoch 8 ======================>\n",
      "1685.0640798807144\n",
      "<====================== Epoch 9 ======================>\n",
      "1568.052683711052\n",
      "<====================== Epoch 10 ======================>\n",
      "1461.1664459109306\n",
      "<====================== Epoch 11 ======================>\n",
      "1366.8325026631355\n",
      "<====================== Epoch 12 ======================>\n",
      "1279.8108813762665\n",
      "<====================== Epoch 13 ======================>\n",
      "1204.1760628819466\n",
      "<====================== Epoch 14 ======================>\n",
      "1141.2586366534233\n",
      "<====================== Epoch 15 ======================>\n",
      "1082.0844423174858\n",
      "<====================== Epoch 16 ======================>\n",
      "1019.8563286960125\n",
      "<====================== Epoch 17 ======================>\n",
      "967.3558802604675\n",
      "<====================== Epoch 18 ======================>\n",
      "927.601091325283\n",
      "<====================== Epoch 19 ======================>\n",
      "887.2434900999069\n",
      "<====================== Epoch 20 ======================>\n",
      "854.3406876921654\n",
      "1:45:52.556360\n",
      "\n",
      "Processing trait  O ...\n",
      "Number of matched label words:  635\n",
      "Top 10 matched label words:  [('great', 1553), ('happy', 1276), ('nice', 1207), ('able', 998), ('funny', 779), ('reason', 706), ('cold', 516), ('easy', 504), ('free', 501), ('ready', 463)]\n",
      "Number of sents containing label words:  23433\n",
      "Top 10 sents containing label words:  [[\"i'm nervous\", 'nervous'], ['you look so young', 'young'], ['very nice touch', 'nice'], [\"these'll go great in my new place\", 'great'], ['based on serious stuff', 'serious'], [\"i'd be able to be a stand-up guy and go the distance\", 'able'], [\"i don't mean to be disrespectful\", 'disrespectful'], ['this is great', 'great'], ['very sexy', 'sexy'], ['joey gets her something really great', 'great']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct Training Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second phase Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_141613/4087040875.py:76: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3bea6924ef4dbdb2134afde9fd591c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "3213.8445814847946\n",
      "<====================== Epoch 2 ======================>\n",
      "2772.5325511693954\n",
      "<====================== Epoch 3 ======================>\n",
      "2521.254425048828\n",
      "<====================== Epoch 4 ======================>\n",
      "2305.2346140146255\n",
      "<====================== Epoch 5 ======================>\n",
      "2122.528953373432\n",
      "<====================== Epoch 6 ======================>\n",
      "1968.2328466176987\n",
      "<====================== Epoch 7 ======================>\n",
      "1823.2791233062744\n",
      "<====================== Epoch 8 ======================>\n",
      "1692.8604972958565\n",
      "<====================== Epoch 9 ======================>\n",
      "1578.5596642494202\n",
      "<====================== Epoch 10 ======================>\n",
      "1474.279801785946\n",
      "<====================== Epoch 11 ======================>\n",
      "1373.6594161391258\n",
      "<====================== Epoch 12 ======================>\n",
      "1290.5531959533691\n",
      "<====================== Epoch 13 ======================>\n",
      "1211.780412375927\n",
      "<====================== Epoch 14 ======================>\n",
      "1141.7948338389397\n",
      "<====================== Epoch 15 ======================>\n",
      "1077.1719652414322\n",
      "<====================== Epoch 16 ======================>\n",
      "1023.2682588994503\n",
      "<====================== Epoch 17 ======================>\n",
      "978.5466766059399\n",
      "<====================== Epoch 18 ======================>\n",
      "932.1611191928387\n",
      "<====================== Epoch 19 ======================>\n",
      "894.0443823933601\n",
      "<====================== Epoch 20 ======================>\n",
      "857.4327992796898\n",
      "1:46:10.771707\n",
      "\n",
      "Processing trait  N ...\n",
      "Number of matched label words:  642\n",
      "Top 10 matched label words:  [('great', 1553), ('happy', 1276), ('nice', 1207), ('able', 998), ('funny', 779), ('reason', 706), ('cold', 516), ('easy', 504), ('free', 501), ('ready', 463)]\n",
      "Number of sents containing label words:  23369\n",
      "Top 10 sents containing label words:  [[\"i'm nervous\", 'nervous'], ['you look so young', 'young'], ['very nice touch', 'nice'], [\"these'll go great in my new place\", 'great'], ['based on serious stuff', 'serious'], [\"i'd be able to be a stand-up guy and go the distance\", 'able'], [\"i don't mean to be disrespectful\", 'disrespectful'], ['this is great', 'great'], ['very sexy', 'sexy'], ['joey gets her something really great', 'great']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct Training Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second phase Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_141613/4087040875.py:76: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae0fe2bc06c4de4b9691124712c0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "3186.9186861515045\n",
      "<====================== Epoch 2 ======================>\n",
      "2754.942104101181\n",
      "<====================== Epoch 3 ======================>\n",
      "2507.4792774915695\n",
      "<====================== Epoch 4 ======================>\n",
      "2293.4754366874695\n",
      "<====================== Epoch 5 ======================>\n",
      "2111.269564270973\n",
      "<====================== Epoch 6 ======================>\n",
      "1951.4770336151123\n",
      "<====================== Epoch 7 ======================>\n",
      "1807.9176329374313\n",
      "<====================== Epoch 8 ======================>\n",
      "1679.0132877230644\n",
      "<====================== Epoch 9 ======================>\n",
      "1563.3644071221352\n",
      "<====================== Epoch 10 ======================>\n",
      "1457.059626340866\n",
      "<====================== Epoch 11 ======================>\n",
      "1363.3138824105263\n",
      "<====================== Epoch 12 ======================>\n",
      "1279.610775232315\n",
      "<====================== Epoch 13 ======================>\n",
      "1199.4739170074463\n",
      "<====================== Epoch 14 ======================>\n",
      "1132.27404910326\n",
      "<====================== Epoch 15 ======================>\n",
      "1070.22325360775\n",
      "<====================== Epoch 16 ======================>\n",
      "1013.7126913070679\n",
      "<====================== Epoch 17 ======================>\n",
      "968.5972692370415\n",
      "<====================== Epoch 18 ======================>\n",
      "922.7060555815697\n",
      "<====================== Epoch 19 ======================>\n",
      "886.5315509438515\n",
      "<====================== Epoch 20 ======================>\n",
      "850.4257380366325\n",
      "1:45:54.675341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "label_words = []\n",
    "for trait in ['A', 'C','E','O','N']:\n",
    "    print('Processing trait ', trait, '...')\n",
    "    label_words = []\n",
    "    with open('label_words/'+trait+'_words.txt', 'r') as f:\n",
    "        pos = f.readline().split(',')\n",
    "        neg = f.readline().split(',')\n",
    "    label_words += pos\n",
    "    label_words += neg\n",
    "    \n",
    "    label_words = set(label_words)\n",
    "    \n",
    "    refined_label_words = label_words - set(('think', 'about', 'really', 'work', 'mean', 'kind'))\n",
    "    \n",
    "    starttime = datetime.datetime.now()\n",
    "    context_dict = {}\n",
    "    label_word_dict = {}\n",
    "\n",
    "    sent_with_label_word = []\n",
    "    for sent in df_data[0]: \n",
    "        for word in refined_label_words: \n",
    "            if word in sent.split(' '):\n",
    "                try:\n",
    "                    label_word_dict[word] += 1\n",
    "                except:\n",
    "                    label_word_dict[word] = 1\n",
    "                sent_with_label_word.append([sent, word])\n",
    "                w_list = sent.split(' ')\n",
    "                for w in w_list:\n",
    "                    if not w == word:\n",
    "                        try:\n",
    "                            context_dict[w] += 1\n",
    "                        except:\n",
    "                            context_dict[w] = 1\n",
    "    \n",
    "    \n",
    "    label_word_dict = {k: v for k, v in sorted(label_word_dict.items(), key=lambda item: abs(item[1]), reverse=True)}\n",
    "    print('Number of matched label words: ', len(label_word_dict))\n",
    "    print('Top 10 matched label words: ', list(label_word_dict.items())[:10])\n",
    "    \n",
    "    print('Number of sents containing label words: ', len(sent_with_label_word))\n",
    "    print('Top 10 sents containing label words: ', sent_with_label_word[:10])\n",
    "    \n",
    "    \n",
    "    \n",
    "    df_data_ = pd.DataFrame([])\n",
    "    df_data_['sent'] = [i[0] for i in sent_with_label_word]\n",
    "    df_data_['label_word'] = [i[1] for i in sent_with_label_word]\n",
    "    \n",
    "    df_data_['span_1'] = df_data_.apply(lambda x: get_span(1, x), axis=1)\n",
    "    df_data_['span_2'] = df_data_.apply(lambda x: get_span(2, x), axis=1)\n",
    "    \n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "    \n",
    "    print('Construct Training Samples...')\n",
    "    train_dataloader = construct_training(df_data_, tokenizer)\n",
    "    \n",
    "    print('Second phase Pre-training...')    \n",
    "    \n",
    "    \n",
    "    num_epoch          = 20\n",
    "    learning_rate      = 1e-4\n",
    "    adam_epsilon       = 1e-8\n",
    "    num_warmup_steps   = 0\n",
    "    num_training_steps = len(train_dataloader)*num_epoch\n",
    "\n",
    "\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate, eps = adam_epsilon, correct_bias = False)  \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  \n",
    "\n",
    "\n",
    "    model.cuda()\n",
    "    for _ in tnrange(1, num_epoch+1, desc='Epoch'):\n",
    "        print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "        batch_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            b_input_ids, b_input_masks, b_labels = batch\n",
    "\n",
    "            loss = model(input_ids=b_input_ids, attention_mask=b_input_masks, labels=b_labels).loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        print(batch_loss)\n",
    "    \n",
    "    model_path = 'Adapted_t5_large_'+trait+'/'\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    endtime = datetime.datetime.now()\n",
    "    print((endtime - starttime))\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wen",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
